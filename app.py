import os
os.environ["HF_HUB_ENABLE_HF_TRANSFER"]='1'
from huggingface_hub import snapshot_download
import json
import numpy as np
import torch
from transformers import pipeline


class InferlessPythonModel:

    # replace ##task_type## and ##huggingface_name## with appropriate values
    def initialize(self):
        model_id = "EleutherAI/gpt-neo-125M"
        snapshot_download(repo_id=model_id,allow_patterns=["*.safetensors"])
        self.generator = pipeline("text-generation", model=model_id,device=0)

    # inputs is a list of dictonary where the keys are input names and values are actual input data
    # e.g. in the below code the input name is prompt 
    # The output generated by the infer function should be a dictonary where keys are output names and values are actual output data
    # e.g. in the below code the output name is generated_txt
    def infer(self, inputs):
        output = []

        print(" no of inputs to be processed " + str(len(inputs)))
        for each in inputs:
            prompt = each["prompt"]
            pipeline_output = self.generator(prompt, do_sample=True, min_length=20)
            generated_txt = pipeline_output[0]["generated_text"]
            print("generated_txt", generated_txt, flush=True)
            output.append({"generated_text": generated_txt })
        return output

    # perform any cleanup activity here
    def finalize(self,args):
        self.pipe = None

